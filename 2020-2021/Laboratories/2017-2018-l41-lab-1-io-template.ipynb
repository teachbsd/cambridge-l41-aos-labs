{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L41: Lab 1 - Getting started with kernel tracing - I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goals of this lab are to:\n",
    "    \n",
    "- Introduce you to our experimental environment and DTrace.\n",
    "- Have you explore user-kernel interactions via system calls and traps.\n",
    "- Gain experience tracing I/O behaviour in UNIX.\n",
    "- Develop an intuition about DTrace's probe effect.\n",
    "\n",
    "You will do this using DTrace to analyse the behaviour of a potted, kernel-intensive block-I/O benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Block-I/O benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Extract and build the benchmark\n",
    "!make -C io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the benchmark\n",
    "\n",
    "Once built, you can run the benchmark binaries as follows (with the command-line arguments specifying various benchmark parameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Execute the io-static benchmark displaying the command line options\n",
    "!io/io-static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Execute the io-dynamic benchmark displaying the command line options\n",
    "!io/io-dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example benchmark commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example creates a default-sized benchmark data file (the benchmark data file is stored in the `/data` directory on the BeagleBone Black SD card):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Example benchmark command\n",
    "print(\"Creating file to run benchmark\")\n",
    "\n",
    "!io/io-static -c iofile\n",
    "\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/latex"
   },
   "source": [
    "The following example runs a simple `read()` benchmark on the benchmark data file, printing additional information (`-v`) about the benchmark run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Example benchmark command\n",
    "print(\"Running benchmark\")\n",
    "\n",
    "!io/io-static -v -r iofile\n",
    "\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example runs a simple `write()` benchmark on the data file, printing additional information (`-v`) about the benchmark run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Example benchmark command\n",
    "print(\"Running benchmark\")\n",
    "\n",
    "!io/io-static -v -w iofile\n",
    "\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example runs a simple write() benchmark multiple times printing out the average measured I/O bandwidth (KiBytes/sec):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Example benchmark command\n",
    "import numpy as np\n",
    "print(\"Running benchmark\")\n",
    "\n",
    "# Run the benchmark multiple times storing each measure bandwidth in values[]\n",
    "values = []\n",
    "\n",
    "for trials in range(0, 10):\n",
    "    output = !io/io-static -w iofile\n",
    "    print(trials, output)\n",
    "    values.append(float(output[1].split()[1]))\n",
    "    \n",
    "# Compute and print the average I/O bandwidth\n",
    "average_bw = np.mean(values)\n",
    "median_bw = np.median(values)\n",
    "q1_bw = np.quantile(values, 0.25)\n",
    "q3_bw = np.quantile(values, 0.75)\n",
    "print(\"Average bandwidth = {} KiBytes/sec\".format(average_bw))\n",
    "print(\"Q1 bandwidth = {} KyBytes/sec\".format(q1_bw))\n",
    "print(\"Median bandwidth = {} KyBytes/sec\".format(median_bw))\n",
    "print(\"Q3 = {} KyBytes/sec\".format(q3_bw))\n",
    "\n",
    "\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing whole-program analysis using DTrace, be sure to suppress the benchmark's output (bare `-B`) mode as follows (this prevents DTrace from tracing the printing of the benchmark):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Example benchmark\n",
    "print(\"Running benchmark\")\n",
    "\n",
    "!io/io-static -B -q -r iofile\n",
    "\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example measures the performance of the read benchmark when the buffer cache is enabled and disable. As noted in the lab handouts, be sure to discard the ouput of the first run of the benchmark when the buffer cache is disabled (as cached data may still be  accessed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Example benchmark command\n",
    "print(\"Running benchmark\")\n",
    "\n",
    "enabled = !io/io-static -r iofile\n",
    "enabled_again = !io/io-static -r iofile\n",
    "disabled = !io/io-static -d -r iofile\n",
    "\n",
    "print(\"Buf. cache enabled: bandwidth = {} KiBytes/sec\".\n",
    "      format(float(enabled[1].split()[1])))\n",
    "print(\"Buf. cache enabled (2nd run): bandwidth = {} KiBytes/sec\".\n",
    "      format(float(enabled_again[1].split()[1])))\n",
    "print(\"Buf. cache disabled: bandwidth = {} KiBytes/sec\".\n",
    "      format(float(disabled[1].split()[1])))\n",
    "\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a high-level summary of execution time (including a breakdown of total wall-clock time, time in userspace and \"system-time\") use the UNIX time command as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Example benchmark command\n",
    "print(\"Running benchmark\")\n",
    "\n",
    "!/usr/bin/time -p io/io-static -r -B -d -q iofile\n",
    "\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DTrace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentation with DTrace is best performed directly from within the shell:\n",
    "\n",
    "```\n",
    "ssh root@192.168.141.100\n",
    "root@l41-beaglebone:~ # dtrace -n 'BEGIN { print(\"hello world\"); }'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python-dtrace\n",
    "\n",
    "DTrace scripts can be ran within the Jupyter notebook with the assistance of the `python-dtrace` module. Note:  this is somewhat more involved that running the `dtrace` command from the shell as the script must be executed within a seperate thread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example uses DTrace to measure the execution time of the benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# D Language script\n",
    "from dtrace import DTraceConsumerThread\n",
    "io_performance_script = \"\"\"\n",
    "BEGIN {\n",
    "   self->targetPid = -1;\n",
    "}\n",
    "\n",
    "proc:::exec-success\n",
    "/execname == \"io-static\"/\n",
    "{\n",
    "   self->targetPid = pid;\n",
    "   self->start = vtimestamp;\n",
    "}\n",
    "\n",
    "syscall::*exit:entry\n",
    "/pid == self->targetPid/\n",
    "{\n",
    "   self->targetPid = -1;\n",
    "   trace(vtimestamp - self->start);\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "values = []\n",
    "\n",
    "# Callback invoked to print the trace record\n",
    "# (that is, printf(\"%u\", vtimestamp - self->start))\n",
    "def simple_out(value):\n",
    "    print(\"Traced value:\", value)\n",
    "    values.extend([int(x) for x in value.split()])\n",
    "    \n",
    "# Create a seperate thread to run the DTrace instrumentation\n",
    "dtrace_thread = DTraceConsumerThread(io_performance_script,\n",
    "                                    chew_func=lambda v: None,\n",
    "                                    chewrec_func=lambda v: None,\n",
    "                                    out_func=simple_out,\n",
    "                                    walk_func=None,\n",
    "                                    sleep=1)\n",
    "\n",
    "# Start the DTrace instrumentation\n",
    "dtrace_thread.start()\n",
    "\n",
    "# Display header to indicate that the benchmarking has started\n",
    "print(\"Starting io-static read performance measurement\")\n",
    "\n",
    "# Run the io-static benchmark    \n",
    "TOTAL_SIZE = 16*1024*1024\n",
    "BUFFER_SIZE = 8192\n",
    "\n",
    "for trials in range(0, 10):\n",
    "    output = !io/io-static -r -B -q -b {str(BUFFER_SIZE)} -t {str(TOTAL_SIZE)} iofile\n",
    "        \n",
    "# The benchmark has completed - stop the DTrace instrumentation\n",
    "dtrace_thread.stop()\n",
    "dtrace_thread.join()\n",
    "\n",
    "# Compute and print the average time\n",
    "average_time = sum(values[1:])/len(values[1:])\n",
    "print(\"Average time = {} nsec\".format(average_time))\n",
    "print(\"Average bandwidth = {} KiBytes/sec\".format((TOTAL_SIZE/1024)/(average_time/1e9)))\n",
    "\n",
    "# Display footer to indicate that the benchmarking has finished\n",
    "print(\"Finished io-static read performance measurement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example uses DTrace to record the number of times each `syscall` is called by the `io-static` benchmark reading the benchmark data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# D Language script\n",
    "io_syscall_script = \"\"\"\n",
    "syscall:::entry\n",
    "/execname == \"io-static\"/\n",
    "{\n",
    "    @a[probefunc] = count();\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "values = defaultdict(int)\n",
    "\n",
    "# Callback invoked to process the aggregation\n",
    "def simple_walk(action, identifier, keys, value):\n",
    "    \"\"\"\n",
    "    action -- type of action (sum, avg, ...)\n",
    "    identifier -- the id.\n",
    "    keys -- list of keys.\n",
    "    value -- the value.\n",
    "    \"\"\"\n",
    "    values[keys[0]] += value\n",
    "\n",
    "# Create a seperate thread to run the DTrace instrumentation\n",
    "dtrace_thread = DTraceConsumerThread(io_syscall_script,\n",
    "                                     walk_func=simple_walk,\n",
    "                                     out_func=lambda v: None,\n",
    "                                     chew_func=lambda v: None,\n",
    "                                     chewrec_func=lambda v: None,\n",
    "                                     sleep=1)\n",
    "\n",
    "# Start the DTrace instrumentation\n",
    "dtrace_thread.start()\n",
    "\n",
    "# Display header to indicate that the benchmarking has started\n",
    "print(\"Starting io-static read performance measurement\")\n",
    "\n",
    "# Run the io-static benchmark    \n",
    "TOTAL_SIZE = 16*1024*1024\n",
    "BUFFER_SIZE = 512\n",
    "\n",
    "output_dtrace = !io/io-static -r -b {str(BUFFER_SIZE)} -t {str(TOTAL_SIZE)} iofile\n",
    "        \n",
    "# The benchmark has completed - stop the DTrace instrumentation\n",
    "dtrace_thread.stop()\n",
    "dtrace_thread.join()\n",
    "    \n",
    "# Print the syscalls and their frequency\n",
    "print(\"Number of read() calls {}\".format(values[b'read']))\n",
    "\n",
    "# Display footer to indicate that the benchmarking has finished\n",
    "print(\"Finished io-static read performance measurement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example explores the performance impact of adding a predicate clause in a DTrace script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# D Language scripts\n",
    "io_syscall_script = \"\"\"\n",
    "syscall:::entry\n",
    "/execname == \"io-static\"/\n",
    "{\n",
    "    @a[probefunc] = count();\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "io_syscall_script_no_predicate = \"\"\"\n",
    "syscall:::entry\n",
    "{\n",
    "    @a[probefunc] = count();\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "TOTAL_SIZE = 16*1024*1024\n",
    "BUFFER_SIZE = 512\n",
    "values = dict()\n",
    "\n",
    "# Callback invoked to process the aggregation\n",
    "def simple_walk(action, identifier, keys, value):\n",
    "    \"\"\"\n",
    "    action -- type of action (sum, avg, ...)\n",
    "    identifier -- the id.\n",
    "    keys -- list of keys.\n",
    "    value -- the value.\n",
    "    \"\"\"\n",
    "    key = keys[0]\n",
    "    count = value\n",
    "    if key in values:\n",
    "        values[key] += value\n",
    "    else:\n",
    "        values[key] = value\n",
    "\n",
    "# Create a seperate thread to run the DTrace instrumentation\n",
    "dtrace_thread = DTraceConsumerThread(io_syscall_script,\n",
    "                                     chew_func=lambda v: None,\n",
    "                                     chewrec_func=lambda v: None,\n",
    "                                     out_func=lambda v: None,\n",
    "                                     walk_func=simple_walk,\n",
    "                                     sleep=1)\n",
    "\n",
    "# Start the DTrace instrumentation\n",
    "dtrace_thread.start()\n",
    "\n",
    "# Display header to indicate that the benchmarking has started\n",
    "print(\"Starting io-static read performance measurement\")\n",
    "\n",
    "# Run the io-static benchmark    \n",
    "output_predicate = !io/io-static -r -b {str(BUFFER_SIZE)} -t {str(TOTAL_SIZE)} iofile\n",
    "        \n",
    "# The benchmark has completed - stop the DTrace instrumentation\n",
    "dtrace_thread.stop()\n",
    "dtrace_thread.join()\n",
    "\n",
    "# Now run the benchmark again but without the predicate\n",
    "\n",
    "# Create a seperate thread to run the DTrace instrumentation\n",
    "dtrace_thread = DTraceConsumerThread(io_syscall_script_no_predicate,\n",
    "                                     chew_func=None,\n",
    "                                     chewrec_func=None,\n",
    "                                     walk_func=simple_walk,\n",
    "                                     out_func=lambda v: None,\n",
    "                                     sleep=1)\n",
    "\n",
    "# Start the DTrace instrumentation\n",
    "dtrace_thread.start()\n",
    "\n",
    "# Run the io-static benchmark    \n",
    "output_no_predicate = !io/io-static -r -b {str(BUFFER_SIZE)} -t {str(TOTAL_SIZE)} iofile\n",
    "        \n",
    "# The benchmark has completed - stop the DTrace instrumentation\n",
    "dtrace_thread.stop()\n",
    "dtrace_thread.join()\n",
    "\n",
    "# Print the performance of both runs\n",
    "print(\"With predicate {} KiBytes/sec\".format(output_predicate[1].split()[1]))\n",
    "print(\"Without predicate {} KiBytes/sec\".format(output_no_predicate[1].split()[1]))\n",
    "\n",
    "# Display footer to indicate that the benchmarking has finished\n",
    "print(\"Finished io-static read performance measurement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## matplotlib and pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot read performance (against buffer size)\n",
    "\n",
    "The following example plots data read from a previous saved file. The example also demonstrates calcualtion of error bars using Python's `pandas` library. (Generating plots on the BeagleBone Black can be rather slow!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot the read performance (IO bandwidth against buffer size with error bars)\n",
    "with open(\"2017-2018-l41-lab1.data\", 'r') as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "read_performance_values = [v.strip() for v in content]\n",
    "       \n",
    "# Buffer sizes to compute the performance with\n",
    "BUFFER_SIZES = [2048 * 2 ** exp for exp in range(0, 14)]\n",
    "\n",
    "# Total size of iofile (default size) = 16MiB\n",
    "TOTAL_SIZE = BUFFER_SIZES[-1] #16*1024*1024\n",
    "\n",
    "# Number of trials for each buffer size\n",
    "NUM_TRIALS = 11\n",
    "\n",
    "# Compute the IO bandwidth in KiBytes/sec\n",
    "io_bandwidth_values = [(TOTAL_SIZE/1024)/(json.loads(val)[\"timestamp\"]/1e9) for val in read_performance_values]\n",
    "\n",
    "# Reshape the list into an array of size [len(BUFFER_SIZES), NUM_TRIALS]\n",
    "io_bandwidth = np.reshape(io_bandwidth_values, (len(BUFFER_SIZES), NUM_TRIALS))[:,:]\n",
    "\n",
    "# Convert the array of io bandwidth values into a Panda DataFrame\n",
    "# this allows ploting of the median value and computation of the \n",
    "# error bars (25 and 75 percentile values)\n",
    "# Note: The error bars should be small indicating that the experiment is tightly controlled\n",
    "df = pd.DataFrame(io_bandwidth, index=BUFFER_SIZES)\n",
    "\n",
    "# Compute error bars based on the 25 and 75 quartile values\n",
    "error_bars = df.quantile([.25, .75], axis=1)\n",
    "error_bars.loc[[0.25]] = df.median(1) - error_bars.loc[[0.25]]\n",
    "error_bars.loc[[0.75]] = error_bars.loc[[0.75]] - df.median(1)\n",
    "error_bars_values = [error_bars.values]\n",
    "\n",
    "# Create and label the plot\n",
    "plt.figure();\n",
    "df.median(1).plot(figsize=(9,9), yerr=error_bars_values, label=\"io-static read\")\n",
    "plt.title('io-static read performance')\n",
    "plt.ylabel('I/O bandwidth (KiBytes/sec)')\n",
    "plt.xlabel('Buffer size (Bytes)')\n",
    "plt.xscale('log')\n",
    "\n",
    "# Plot a vertical line at 1MiB\n",
    "plt.axvline(x=1024*1024, color='g')\n",
    "\n",
    "# Display the plot and save it to a file\n",
    "# (this can take a while (~30 secs) on the BeagleBone Black)\n",
    "plt.savefig(\"2017-2018-l41-lab1-performance.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}