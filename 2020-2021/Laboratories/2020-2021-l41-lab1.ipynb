{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L41: Lab 1 - Getting started with kernel tracing - I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jypterlab Notebook is intended to get you started with:\n",
    "\n",
    "1. Building and running the benchmark\n",
    "2. Extracting and plotting data collected by the benchmark itself (e.g., execution time)\n",
    "3. Extracting and plotting data collected externally by DTrace\n",
    "\n",
    "This file is not intended to be a template for your solutions; we recommend that you create a new Notebook, placing your work there, rather than using this Notebook as your starting point.\n",
    "\n",
    "Make sure to run cells in the right order (pressing Ctrl-Enter when in the cell) so that dependencies are executed in the right order. For example, Python imports must occur before running the remainder of the code, and data must be collected before it can be plotted.\n",
    "\n",
    "Note: When you execute a cell in Jupyterlab, the bracketed number to the left (e.g., `[1]`) will temporarily change to a `[*]` to indicate that it has not yet completed. Running benchmarks or longer forms of data analysis or plotting may take a considerable time on our RPi4 boards, so do exercise patience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Building and running the benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to build the benchmark using `make` (no text output is expected from a successful build):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make -C io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can run the benchmark using Jupyter's `!` syntax, illustrating its command-line arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!io/io-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a quick test of the benchmark using small parameters so that we can see the JSON format of the output, which you will need to know in order to extract various results of interest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a data file suitable for the I/O benchmark to use; the default parameters are fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!io/io-benchmark -c iofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!io/io-benchmark -b 262144 -g -j -v -n 2 -r iofile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `\"host_configuration\"` and `\"benchmark_configuration\"` blocks provide information about the configuration of the host and the benchmark.\n",
    "\n",
    "The `\"benchmark_samples`\" block consists of an array of individual measurements with various results for each measurement. In general, dropping the first sample is a good idea, as it may contain artifacts from \"first runs\" -- such as the costs of dynamic linking. The captured metrics using this benchmark command line are:\n",
    "\n",
    "- `bandwidth`: The average bandwidth over the run of the benchmark's work loop.\n",
    "- `time`: Wall-clock time running the work loop.\n",
    "- `utime` and `stime`: Sampled user and system (kernel) time. This may not add up to wall-clock time if software has to sleep awaiting I/O. Further, while `time` is measured using precise clock reads, `utime` and `stime` are sampled by the timer interrupt. You therefore cannot expect that (`time` == `utime` + `stime`).\n",
    "- `inblock` and `outblock`: The number of actual block I/O operations performed by the process measured using `getrusage(2)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extracting and plotting data generated by the benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import some Python module dependencies, and set configuration parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Enable Jupyter notebook mode for matplotlib\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set low for experimentation; consider using 11 \"in production\", but this will run for a long time!\n",
    "iterations=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the benchmark and process the results. We run the benchmark binary once for each buffer size, `iterations` iterations each time, generating JSON. We import the JSON into Python, and generate some summary statistics (medians and quartiles) for each buffer size. In this example, we consider only bandwidth, but you can also easily plot properties such as I/O counts or user/system time.\n",
    "\n",
    "You will likely want to modify this code to drop the first sample for each size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_strings = {}\n",
    "print(\"Benchmark run starting\")\n",
    "for buffersize in [2**v for v in range(25)]:\n",
    "    print(\"Buffer size: \", buffersize)\n",
    "    output = !io/io-benchmark -b $buffersize -j -n $iterations -r -v iofile\n",
    "    benchmark_strings[buffersize] = ' '.join(output)\n",
    "display(\"Benchmark run completed\")\n",
    "    \n",
    "bw_samples = {}       # Arrays of bandwidth samples indexed by buffer size\n",
    "medians = {}          # Arrays of medians indexed by buffer size\n",
    "stds = {}             # Arrays of standard deviations indexed by buffer size\n",
    "q3s = {}              # Arrays of third quartiles\n",
    "q1s = {}              # Arrays of first quartiles\n",
    "\n",
    "for buffersize in [2**v for v in range(25)]:\n",
    "    j = json.loads(benchmark_strings[buffersize])\n",
    "    samples = list(j[\"benchmark_samples\"])\n",
    "    bw_samples[buffersize] = list([x[\"bandwidth\"] for x in samples])\n",
    "    medians[buffersize] = np.median(bw_samples[buffersize])\n",
    "    q1s[buffersize] = medians[buffersize] - np.quantile(bw_samples[buffersize], 0.25)\n",
    "    q3s[buffersize] = np.quantile(bw_samples[buffersize], 0.75) - medians[buffersize] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we generate a plot using `matplotlib`, consisting of medians and error bars based on IQR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax = plt.subplots()\n",
    "ax.set_title(\"buffer size vs. bandwidth\")\n",
    "\n",
    "x_coords = []\n",
    "y_coords = []\n",
    "low_errs = []\n",
    "high_errs = []\n",
    "\n",
    "for x in [2**v for v in range(25)]:\n",
    "    x_coords.append(x)\n",
    "    y_coords.append(medians[x])\n",
    "    low_errs.append(q1s[x])\n",
    "    high_errs.append(q3s[x])\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.errorbar(x_coords, y_coords, [low_errs, high_errs])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In analysing this plot, it is worth considering key inflection points: Points on the plot where there are behavioural changes, and what they reflect. We can directly annotate those points on the plot using `avxline`.\n",
    "\n",
    "In the next plot, we've manually placed several vertical lines at points where the data you collect is likely to experience inflection points. If they don't line up, check that you are collecting data as expected.\n",
    "\n",
    "Be sure to take note of the linear Y axis and exponential X axis, and consider its implications for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This content the same as the above cell\n",
    "fig1, ax = plt.subplots()\n",
    "ax.set_title(\"buffer size vs. bandwidth\")\n",
    "\n",
    "x_coords = []\n",
    "y_coords = []\n",
    "low_errs = []\n",
    "high_errs = []\n",
    "\n",
    "for x in [2**v for v in range(25)]:\n",
    "    x_coords.append(x)\n",
    "    y_coords.append(medians[x])\n",
    "    low_errs.append(q1s[x])\n",
    "    high_errs.append(q3s[x])\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.errorbar(x_coords, y_coords, [low_errs, high_errs])\n",
    "\n",
    "### This is new content relative to the prior cell\n",
    "ax.axvline(x=4*1024, color=\"red\", label=\"4KB\", linestyle=\":\")\n",
    "ax.axvline(x=64*1024, color=\"blue\", label=\"64KB\", linestyle=\":\")\n",
    "ax.axvline(x=128*1024, color=\"green\", label=\"128KB\", linestyle=\":\")\n",
    "ax.legend()\n",
    "ax.errorbar(x_coords, y_coords, [low_errs, high_errs])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save a plot out to disk as a PDF -- e.g., for use in a lab report -- using this API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.savefig(\"performance.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extracting and plotting data generated using DTrace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DTrace scripts can be run directly from Python and Jupyter, returning a data structure that describes the resulting output. The details of the data structure depend on the script you have written. The DTrace script will run asynchronously while the benchmark runs, and you then collect the data after completion.\n",
    "\n",
    "You will likely wish to develop DTrace scripts using the `dtrace(1)` command-line tool rather than in Python, as that will give more ready access to debug output (such as script compilation failure details).\n",
    "\n",
    "First you need to import the `python-dtrace` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtrace import DTraceConsumerThread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example uses DTrace to record the number of times each `syscall` is called by the `io-benchmark` benchmark reading the benchmark data file. Note that it brackets data collection based on both the executable name (`io-benchmark`) and also the start and finish of the benchmark loop as detected using calls to the `clock_gettime(2)` system call (note that the system call is invoked directly in `io-benchmark` to bypass `vdso` optimisation). We set iterations to 1 to avoid capturing data from more than one run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D Language script\n",
    "io_syscall_script = \"\"\"\n",
    "syscall::clock_gettime:return\n",
    "/execname == \"io-benchmark\" && !in_benchmark/\n",
    "{\n",
    "    in_benchmark = 1;\n",
    "}\n",
    "\n",
    "syscall::clock_gettime:entry\n",
    "/execname == \"io-benchmark\" && in_benchmark/\n",
    "{\n",
    "    in_benchmark = 0;\n",
    "}\n",
    "\n",
    "syscall:::entry\n",
    "/execname == \"io-benchmark\" && in_benchmark && probefunc != \"clock_gettime\"/\n",
    "{\n",
    "    @a[probefunc] = count();\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "values = defaultdict(int)\n",
    "\n",
    "# Callback invoked to process the aggregation\n",
    "def simple_walk(action, identifier, keys, value):\n",
    "    \"\"\"\n",
    "    action -- type of action (sum, avg, ...)\n",
    "    identifier -- the id.\n",
    "    keys -- list of keys.\n",
    "    value -- the value.\n",
    "    \"\"\"\n",
    "    values[keys[0]] += value\n",
    "\n",
    "# Create a seperate thread to run the DTrace instrumentation\n",
    "dtrace_thread = DTraceConsumerThread(io_syscall_script,\n",
    "                                     walk_func=simple_walk,\n",
    "                                     out_func=lambda v: None,\n",
    "                                     chew_func=lambda v: None,\n",
    "                                     chewrec_func=lambda v: None,\n",
    "                                     sleep=1)\n",
    "\n",
    "# Start the DTrace instrumentation\n",
    "dtrace_thread.start()\n",
    "\n",
    "# Display header to indicate that the benchmarking has started\n",
    "print(\"Starting io-benchmark read performance measurement\")\n",
    "\n",
    "# Run the io-benchmark benchmark    \n",
    "BUFFER_SIZE = 512\n",
    "\n",
    "output_dtrace = !io/io-benchmark -r -b {str(BUFFER_SIZE)} iofile\n",
    "        \n",
    "# The benchmark has completed - stop the DTrace instrumentation\n",
    "dtrace_thread.stop()\n",
    "dtrace_thread.join()\n",
    "    \n",
    "# Print the syscalls and their frequency\n",
    "for x in values.keys():\n",
    "    print(\"Number of \", x, \" calls {}\", values[x])\n",
    "\n",
    "# Display footer to indicate that the benchmarking has finished\n",
    "print(\"Finished io-benchmark read performance measurement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach can be used to extract a variety of kernel trace data using DTrace. One known limitation is that stack() results are stored as a set of code addresses, rather than being expanded to strings, so you will likely prefer to use the `dtrace(1)` command-line tool to capture stack data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}