{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Operating Systems: Lab 1 - Getting started with kernel tracing - I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jypterlab Notebook is intended to get you started with:\n",
    "\n",
    "1. Building and running the benchmark\n",
    "2. Extracting and plotting data collected by the benchmark itself (e.g., execution time)\n",
    "3. Extracting data collected externally by DTrace\n",
    "\n",
    "This file is not intended to be a template for your solutions; we recommend that you create a new Notebook, placing your work there, copying and pasting template code from this Notebook as seems useful.\n",
    "\n",
    "You can add new cells to your notebook using the '+' button on the panel above.  You can change whether a cell is treated as Python or Markdown using the selector above (it defaults to 'Code').\n",
    "\n",
    "Make sure to run cells in the right order (pressing Ctrl-Enter when in the cell to run it) so that dependencies are executed in the right order. For example, Python imports must occur before running the remainder of the code, and data must be collected before it can be plotted.  If you restart Jupyterlab, the Python kernel will restart, and one-time Python intialisation must be performed again (e.g., imports and data collection).\n",
    "\n",
    "**Note**: When you execute a cell in Jupyterlab, the bracketed number to the left (e.g., `[1]`) will temporarily change to a `[*]` to indicate that it has not yet completed executin. Running benchmarks or longer forms of data analysis or plotting may take a considerable time on our RPi4 boards, so do exercise patience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Building and running the benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, using Jupyter's `!` syntax, build the benchmark using `make`; if no dependencies have changed, you may get no output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make -C io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The benchmark command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can run the benchmark tool without command-line arguments to get usage information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!io/io-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Not all of the benchmark modes and options are relevant to this year's assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the benchmark data file (one time only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a data file suitable for the I/O benchmark to use. This needs to be done only once. Use a total file size of 16MiB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!io/io-benchmark -t 16777216 create iofile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Describe the hardware, OS, and default benchmark configuration\n",
    "\n",
    "The benchmark tool's `describe` command provides more detailed about the hardware, OS, and benchmark configuration. This information will also be generated by the other commands if you specify `-v` with them, which can be useful to capture duriing benchmark runs to validate the configuration or use as input to data analysis or plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!io/io-benchmark describe iofile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine-readable output\n",
    "\n",
    "With the `-j` option, the benchmark tool will generate all output including configuration information and benchmark results as JSON. Using Python JSON support (e.g., `json.loads`), you can avoid manual parsing to analyse data in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!io/io-benchmark -j describe iofile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example benchmark run\n",
    "\n",
    "This command line runs the benchmark with a block size of 256KiB (`-b 262144`), total I/O size of 16MiB (`-t 16777216`), captures rusage information (`-g`), prints the results in JSON (`-j`), includes additional configuration information (`-v`), runs two iterations (`-n 2`), and executes a read workload (`read`) on the previously created data file `iofile`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!io/io-benchmark -b 262144 -t 16777216 -g -j -v -n 2 read iofile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `\"hw_configuration\"`, `\"os_configuration\"`, and `\"benchmark_configuration\"` blocks provide information about the configuration of the host and the benchmark as above.  Remove `-v` from the command line to omit this information from benchmark runs.\n",
    "\n",
    "The `\"benchmark_samples\"` block consists of an array of individual measurement results with various metrics for each measurement. In general, dropping the first sample is a good idea, as it may contain artifacts from \"first runs\" -- such as the one-time costs of lazy dynamic linking. The captured metrics using this benchmark command line include:\n",
    "\n",
    "- `bandwidth`: The average bandwidth over the run of the benchmark's work loop.\n",
    "- `time`: Wall-clock time running the work loop.\n",
    "- `utime` and `stime`: Sampled user and system (kernel) time. This may not add up to wall-clock time if software has to sleep awaiting I/O. Further, while `time` is measured using precise clock reads, `utime` and `stime` are sampled by the timer interrupt. You therefore cannot expect that (`time` == `utime` + `stime`).\n",
    "- `inblock` and `outblock`: The number of actual block I/O operations performed by the process measured using `getrusage(2)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generating, extracting and plotting data generated by the benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import Python module dependencies and set configuration parameters. This needs to be done once per session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Enable Jupyter notebook mode for matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set low for experimentation; consider using 7 \"in production\", but that will take a lot longer!\n",
    "iterations=3\n",
    "logminbufsize=5\n",
    "logmaxbufsize=25\n",
    "\n",
    "buffersizes = [2**v for v in range(logminbufsize, logmaxbufsize)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run benchmark and collect results in Python data structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the benchmark and digest the results. We run the benchmark binary once for each buffer size, requesting it execute `iterations` iterations each time, generating JSON output. We import the JSON into Python, and generate some summary statistics (medians and quartiles) for each buffer size. In this example, we consider only bandwidth, but you can also easily plot properties such as I/O counts or user/system time (including distribution properties such as IQR).\n",
    "\n",
    "**Note**: You will likely want to modify this code to drop the first sample for each size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_strings = {}\n",
    "print(\"Benchmark run starting\")\n",
    "for buffersize in buffersizes:\n",
    "    print(\"Buffer size: \", buffersize)\n",
    "    output = !io/io-benchmark -b $buffersize -t 16777216 -j -n $iterations -v read iofile\n",
    "    benchmark_strings[buffersize] = ' '.join(output)\n",
    "print(\"Benchmark run completed\")\n",
    "    \n",
    "bw_samples = {}       # Arrays of bandwidth samples indexed by buffer size\n",
    "medians = {}          # Arrays of medians indexed by buffer size\n",
    "stds = {}             # Arrays of standard deviations indexed by buffer size\n",
    "q3s = {}              # Arrays of third quartiles\n",
    "q1s = {}              # Arrays of first quartiles\n",
    "\n",
    "for buffersize in buffersizes:\n",
    "    j = json.loads(benchmark_strings[buffersize])\n",
    "    samples = list(j[\"benchmark_samples\"])\n",
    "    bw_samples[buffersize] = list([x[\"bandwidth\"] for x in samples])\n",
    "    medians[buffersize] = np.median(bw_samples[buffersize])\n",
    "    q1s[buffersize] = medians[buffersize] - np.quantile(bw_samples[buffersize], 0.25)\n",
    "    q3s[buffersize] = np.quantile(bw_samples[buffersize], 0.75) - medians[buffersize] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the collected data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we generate a plot using `matplotlib`, consisting of medians and error bars based on IQR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax = plt.subplots()\n",
    "ax.set_title(\"buffer size vs. bandwidth\")\n",
    "\n",
    "x_coords = []\n",
    "y_coords = []\n",
    "low_errs = []\n",
    "high_errs = []\n",
    "\n",
    "for x in buffersizes:\n",
    "    x_coords.append(x)\n",
    "    y_coords.append(medians[x])\n",
    "    low_errs.append(q1s[x])\n",
    "    high_errs.append(q3s[x])\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.errorbar(x_coords, y_coords, [low_errs, high_errs])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an annotated plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In analysing this plot, it is worth considering key inflection points: Points on the plot where there are behavioural changes, and what they reflect. We can directly annotate those points on the plot using `avxline`.\n",
    "\n",
    "In the next plot, we've manually placed several vertical lines at OS and microarchitectural thresholds where the data you collect might experience inflection points.\n",
    "\n",
    "Be sure to take note of the linear Y axis and exponential X axis, and consider its implications for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This content the same as the above cell\n",
    "fig1, ax = plt.subplots()\n",
    "ax.set_title(\"buffer size vs. bandwidth\")\n",
    "\n",
    "x_coords = []\n",
    "y_coords = []\n",
    "low_errs = []\n",
    "high_errs = []\n",
    "\n",
    "for x in buffersizes:\n",
    "    x_coords.append(x)\n",
    "    y_coords.append(medians[x])\n",
    "    low_errs.append(q1s[x])\n",
    "    high_errs.append(q3s[x])\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.errorbar(x_coords, y_coords, [low_errs, high_errs])\n",
    "\n",
    "### This is new content relative to the prior cell\n",
    "ax.axvline(x=4*1024, color=\"red\", label=\"4KB\", linestyle=\":\")\n",
    "ax.axvline(x=64*1024, color=\"blue\", label=\"64KB\", linestyle=\":\")\n",
    "ax.axvline(x=128*1024, color=\"green\", label=\"128KB\", linestyle=\":\")\n",
    "ax.legend()\n",
    "ax.errorbar(x_coords, y_coords, [low_errs, high_errs])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save a plot out to disk as a PDF -- e.g., for use in a lab report -- using this API, which should be called just prior to `plt.show()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.savefig(\"performance.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extracting and plotting data generated using DTrace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DTrace scripts can be run directly from Python and Jupyter, returning a data structure that describes the resulting output. The details of the data structure depend on the script you have written. The DTrace script will run asynchronously while the benchmark runs, and you then collect the data after completion.\n",
    "\n",
    "You will likely wish to develop DTrace scripts using the `dtrace(1)` command-line tool rather than in Python, as that will give more ready access to debug output (such as script compilation failure details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the DTrace module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the `python-dtrace` module; this must be done once per session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtrace import DTraceConsumerThread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect system-call data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example uses DTrace to record the number of times each `syscall` is called by the `io-benchmark` benchmark reading the benchmark data file. Note that it brackets data collection based on both the executable name (`io-benchmark`) and also the start and finish of the benchmark loop as detected using calls to the `clock_gettime(2)` system call (note that the system call is invoked directly in `io-benchmark` to bypass `vdso` optimisation). We set iterations to 1 to avoid capturing data from more than one run. When you run experiments in the lab, you will want to run this for each applicable buffer size, in order to produce plots similar to the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample D-language script embedded in a Python string.\n",
    "# Note that the 'done' variable assumes one benchmark iteration.\n",
    "# You will need to extend this script to use it with multiple iterations.\n",
    "io_syscall_script = \"\"\"\n",
    "\n",
    "/*\n",
    " * Instrument clock_gettime(2) system-call return.  We will use this to detect\n",
    " * the start of the benchmark's work loop.\n",
    " */\n",
    "syscall::clock_gettime:return\n",
    "/execname == \"io-benchmark\" && !self->in_benchmark && !self->done/\n",
    "{\n",
    "    self->in_benchmark = 1;\n",
    "}\n",
    "\n",
    "/*\n",
    " * Intrument clock_gettime() system-call entry.  We will use this to detect the\n",
    " * end of the benchmark's work loop.\n",
    " */\n",
    "syscall::clock_gettime:entry\n",
    "/execname == \"io-benchmark\" && self->in_benchmark  && !self->done/\n",
    "{\n",
    "    self->in_benchmark = 0;\n",
    "    self->done = 1;\n",
    "}\n",
    "\n",
    "/*\n",
    " * Instrument entry to every system call except clock_gettime(2), capturing a\n",
    " * count of the number of times each call is made.\n",
    " */\n",
    "syscall:::entry\n",
    "/execname == \"io-benchmark\" && self->in_benchmark && probefunc != \"clock_gettime\"/\n",
    "{\n",
    "    @a[probefunc] = count();\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "values = defaultdict(int)\n",
    "\n",
    "# Python-DTrace executes the DTrace script asynchronously in another thread.\n",
    "# It uses a set of possible callbacks to return aggregation data to Python.\n",
    "def simple_walk(action, identifier, keys, value):\n",
    "    \"\"\"\n",
    "    action -- type of action (sum, avg, ...)\n",
    "    identifier -- the id.\n",
    "    keys -- list of keys.\n",
    "    value -- the value.\n",
    "    \"\"\"\n",
    "    values[keys[0]] += value\n",
    "\n",
    "# Create a seperate thread to run the DTrace instrumentation\n",
    "dtrace_thread = DTraceConsumerThread(io_syscall_script,\n",
    "                                     walk_func=simple_walk,\n",
    "                                     out_func=lambda v: None,\n",
    "                                     chew_func=lambda v: None,\n",
    "                                     chewrec_func=lambda v: None,\n",
    "                                     sleep=1)\n",
    "\n",
    "# Start the DTrace instrumentation\n",
    "dtrace_thread.start()\n",
    "\n",
    "# Display header to indicate that the benchmarking has started\n",
    "print(\"Starting io-benchmark read performance measurement\")\n",
    "\n",
    "# Run the io-benchmark benchmark    \n",
    "BUFFER_SIZE = 512\n",
    "\n",
    "output_dtrace = !io/io-benchmark -b {str(BUFFER_SIZE)} read iofile\n",
    "        \n",
    "# The benchmark has completed - stop the DTrace instrumentation\n",
    "dtrace_thread.stop()\n",
    "dtrace_thread.join()\n",
    "    \n",
    "# Print the syscalls and their frequency\n",
    "for x in values.keys():\n",
    "    print(\"Number of \", x, \" calls: \", values[x])\n",
    "\n",
    "# Display footer to indicate that the benchmarking has finished\n",
    "print(\"Finished io-benchmark read performance measurement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach can be used to extract a variety of kernel trace data using DTrace. One known limitation is that stack() results are stored as a set of code addresses, rather than being expanded to strings, so you will likely prefer to use the `dtrace(1)` command-line tool to capture stack data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
