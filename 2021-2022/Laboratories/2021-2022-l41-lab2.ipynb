{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Operating Systems\n",
    "# Lab 2 - Inter-Process Communication (IPC)\n",
    "\n",
    "This notebook provides an initial data collection framework for the `ipc-benchmark` benchmark used in Lab 2 (and also later in Lab 3) for a range of IPC analysis tasks.  This includes illustrating use of the benchmark's integrated support for hardware performance counters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference material\n",
    "Here are some useful sources of information as you dig into this lab:\n",
    "\n",
    "- [Raspberry Pi 4 - Broadcom 2711 (BCM2711)](https://www.raspberrypi.org/documentation/hardware/raspberrypi/bcm2711/README.md)\n",
    "- [ARM® Cortex®-A72 MPCore Processor - see section 11.8](https://developer.arm.com/documentation/100095/0003)\n",
    "- [Arm Architecture Reference Manual Armv8, for Armv8-A architecture profile - see sections D7.11.3 and Appendix K3](https://developer.arm.com/documentation/ddi0487/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the benchmark\n",
    "\n",
    "First, using Jupyter's `!` syntax, build the benchmark using `make`; if no dependencies have changed, you may get no output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make -C ipc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Benchmark usage information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ipc/ipc-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware, OS, and default benchmark configuration\n",
    "\n",
    "The `describe` argument causes the benchmark to display hardware, kernel, and network/IPC configuration information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ipc/ipc-benchmark describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect IPC performance data\n",
    "We run the benchmark multiple times (dropping the first sample as the benchmark \"settles\"), across a range of buffer sizes, and capture the results as JSON. The resulting associative array of output strings, `benchmark_output` is indexed by buffer size, with each array entry containing a JSON blob for that run of the benchmark.  IPC type is another parameters in our experiments, but the mode (2-process) will be held constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statistics as stats\n",
    "\n",
    "# Number of iterations to run in each configuration; first will be dropped. Set low for experimentation; consider using 11 \"in production\"\n",
    "iterations=3\n",
    "\n",
    "# Log(2) of the minimum buffer size to use\n",
    "logminbufsize=5     # 32 bytes\n",
    "# Log(2) of the maximum buffer size to use\n",
    "logmaxbufsize=25    # 16M default\n",
    "\n",
    "# Total data to transit over IPC or TCP/IP\n",
    "#totalsize=16777216\n",
    "totalsize=1<<(logmaxbufsize-1)\n",
    "\n",
    "# The list of buffer sizes to iterate over when doing data processing, plotting, etc.\n",
    "buffersizes = [2**v for v in range(logminbufsize, logmaxbufsize)]\n",
    "\n",
    "benchmark_output = {}\n",
    "print(\"Running IPC benchmark with\", iterations, \"iterations for each buffer size; this will take several minutes\")\n",
    "for buffersize in buffersizes:\n",
    "    print(\"  Running set with buffersize\", buffersize);\n",
    "    output = !ipc/ipc-benchmark -i pipe -t $totalsize -b $buffersize -j -P dcache -n $iterations -g -v 2proc\n",
    "    benchmark_output[buffersize] = ' '.join(output)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate summary statistics for plotting\n",
    "We construct a set of arrays of summary statistics, indexed by block size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the JSON for each sample so that we can determine what metrics to plot\n",
    "# We will at least receive bandwidth and time, but may also get performance\n",
    "# counter data to be plotted on the same blocksize x axis.  The resulting dict\n",
    "# 'benchmark_samples' is indexed by block size.\n",
    "benchmark_samples = {}\n",
    "for buffersize in buffersizes:\n",
    "    j = json.loads(benchmark_output[buffersize])\n",
    "    benchmark_samples[buffersize] = list(j[\"benchmark_samples\"])\n",
    "    \n",
    "# Extract primary measurement: bandwidth\n",
    "bw_samples = {}          # Array of bandwidth samples indexed by buffer size\n",
    "bw_medians = {}          # Array of medians indexed by buffer size\n",
    "bw_stds = {}             # Array of standard deviations indexed by buffer size\n",
    "bw_uppererr = {}         # Array of Q3-median indexed by buffer size\n",
    "bw_lowererr = {}         # Array of median-Q1 indexed by buffer size\n",
    "\n",
    "for buffersize in buffersizes:\n",
    "    bw_samples[buffersize] = list([x[\"bandwidth\"] for x in benchmark_samples[buffersize]])\n",
    "    bw_medians[buffersize] = stats.median(bw_samples[buffersize])\n",
    "    bw_stds[buffersize] = stats.stdev(bw_samples[buffersize])\n",
    "    bw_lowererr[buffersize] = bw_medians[buffersize] - np.quantile(bw_samples[buffersize], 0.25)\n",
    "    bw_uppererr[buffersize] = np.quantile(bw_samples[buffersize], 0.75) - bw_medians[buffersize]\n",
    "\n",
    "# Generate summary statistics for directly measured data -- each hash table\n",
    "# is indexed first by metric, and then buffer size.\n",
    "metric_list = list(benchmark_samples[buffersizes[0]][0].keys())\n",
    "#metric_list.remove(\"bandwidth\")\n",
    "#metric_list.remove(\"time\")\n",
    "\n",
    "print(\"Metrics found:\", metric_list)\n",
    "\n",
    "metric_samples = {}\n",
    "metric_medians = {}\n",
    "metric_stds = {}\n",
    "metric_lowererr = {}\n",
    "metric_uppererr = {}\n",
    "for metric in metric_list:\n",
    "    metric_samples[metric] = {}\n",
    "    metric_medians[metric] = {}\n",
    "    metric_stds[metric] = {}\n",
    "    metric_lowererr[metric] = {}\n",
    "    metric_uppererr[metric] = {}\n",
    "    for buffersize in buffersizes:\n",
    "        s = list([float(x[metric]) for x in benchmark_samples[buffersize]])\n",
    "        metric_samples[metric][buffersize] = s\n",
    "        metric_medians[metric][buffersize] = stats.median(s)\n",
    "        metric_stds[metric][buffersize] = stats.stdev(s)\n",
    "        metric_lowererr[metric][buffersize] = metric_medians[metric][buffersize] - np.quantile(s, 0.25)\n",
    "        metric_uppererr[metric][buffersize] = np.quantile(s, 0.75) - metric_medians[metric][buffersize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot resulting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(metric_list), 1, sharex=True)\n",
    "fig.set_size_inches(12, 4 * len(metric_list))\n",
    "\n",
    "# X axis shared by all plots\n",
    "x_coords = buffersizes\n",
    "\n",
    "ax_num = 0\n",
    "for metric in metric_list:   \n",
    "    # Prepare this specific data set -- derive summary statistics, etc.\n",
    "    y_coords = []\n",
    "    errbars = []\n",
    "    lower_errbar = []\n",
    "    upper_errbar = []\n",
    "    for x in buffersizes:\n",
    "        y_coords.append(metric_medians[metric][x])\n",
    "        errbars.append(metric_stds[metric][x])\n",
    "        lower_errbar.append(metric_lowererr[metric][x])\n",
    "        upper_errbar.append(metric_uppererr[metric][x])\n",
    "\n",
    "    ax = axes[ax_num]\n",
    "    ax.errorbar(x_coords, y_coords, [lower_errbar, upper_errbar])\n",
    "    # Logarithmic X axis as we sample at powers of 2.  Care required,\n",
    "    # as it can be visually misleading.  Put ticks are those powers of\n",
    "    # 2 to remind the reader.\n",
    "    ax.set_xscale(\"log\", base=2)\n",
    "    ax.set_xticks(buffersizes)\n",
    "    ax.set_xlabel(\"buffer size (log)\")\n",
    "\n",
    "    # Do a bit of rewriting to make axis labels more readable.\n",
    "    if (metric == \"L2D_CACHE_HIT_RATE\"):\n",
    "        label = \"L2D_HIT_RATE\"\n",
    "    elif (metric == \"L1D_CACHE_HIT_RATE\"):\n",
    "        label = \"L1D_HIT_RATE\"\n",
    "    elif (metric == \"CYCLES_PER_INSTRUCTION\"):\n",
    "        label = \"CPI\"\n",
    "    else:\n",
    "        label = metric\n",
    "\n",
    "    # Select y axis properties based on the kind of metric it is.\n",
    "    if (\"RATE\" in metric or \"PER\" in metric or \"time\" in metric or \"bandwidth\" in metric):\n",
    "        label = label + \" (linear)\"\n",
    "        ax.set_yscale(\"linear\")\n",
    "        ax.set_ylim(ymin=0)\n",
    "    else:\n",
    "        label = label + \" (log)\"\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.set_ylim(ymin=1)\n",
    "    if (\"RATE\" in metric):\n",
    "        ax.set_ylim(ymax=1)\n",
    "        \n",
    "    ax.set_ylabel(label)\n",
    "        \n",
    "    # Draw some vertical lines at key hardware and software thresholds.\n",
    "    # In some cases, they will fall between samples.\n",
    "    ax.axvline(x=8*1024, color=\"pink\", label=\"PIPE_MINDIRECT\", linestyle=\":\")\n",
    "    \n",
    "    ax.axvline(x=32*1024, color=\"grey\", label=\"L1D-CACHE\", linestyle=\"-\")\n",
    "    ax.axvline(x=48*1024, color=\"grey\", label=\"L1I-CACHE\", linestyle=\"--\")\n",
    "    ax.axvline(x=1024*1024, color=\"grey\", label=\"L2D-CACHE\", linestyle=\"-.\")\n",
    "    \n",
    "    ax.axvline(x=32*4096, color=\"tan\", label=\"L1D-TLB\", linestyle=\"-\")\n",
    "    ax.axvline(x=48*4096, color=\"tan\", label=\"L1I-TLB\", linestyle=\"--\")\n",
    "    ax.axvline(x=1024*4096, color=\"tan\", label=\"L2-TLB\", linestyle=\"-.\")\n",
    "    \n",
    "    if (ax_num == 0):\n",
    "        ax.legend()\n",
    "\n",
    "    ax_num += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
